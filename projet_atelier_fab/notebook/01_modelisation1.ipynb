{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "846bfca4-59cc-4121-9ee8-c837e1f79e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cellule 1 — Imports\n",
    "#But : importer les librairies nécessaires (NumPy pour les calculs, random pour le hasard).\n",
    "\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b17eccd-9734-4f99-911c-05ea77a5e777",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cellule 2 — Classe de l’environnement simple\n",
    "\n",
    "#But : créer l’atelier simplifié (pas encore version Gym), dans lequel les stocks évoluent selon les actions.\n",
    "# Cette cellule définit l’atelier : \n",
    "#comment les stocks changent selon l’action et comment l’environnement avance d’un pas.\n",
    "\n",
    "class SimpleWorkshopEnv:\n",
    "    def __init__(self, max_steps=50):\n",
    "        self.max_steps = max_steps\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Stock initial aléatoire entre 0 et 10\n",
    "        self.stock_raw = np.random.randint(0, 11)\n",
    "\n",
    "        # Toujours 0 produit fini au départ\n",
    "        self.stock_sell = 0\n",
    "\n",
    "        self.steps = 0\n",
    "        return (self.stock_raw, self.stock_sell)\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "\n",
    "        if action == 1:  # Produire Produit 1\n",
    "            if self.stock_raw >= 1:\n",
    "                self.stock_raw -= 1\n",
    "                self.stock_sell += 1\n",
    "                reward = 2\n",
    "\n",
    "        elif action == 2:  # Produire Produit 2\n",
    "            if self.stock_raw >= 2:\n",
    "                self.stock_raw -= 2\n",
    "                self.stock_sell += 1\n",
    "                reward = 20\n",
    "\n",
    "        elif action == 3:  # Commander du stock\n",
    "            self.stock_raw += 5\n",
    "            reward = -5\n",
    "\n",
    "        elif action == 0:  # Attendre\n",
    "        # Si on n'a plus de stock, attendre n'a aucun sens → petite pénalité\n",
    "            reward = -1 if self.stock_raw == 0 else 0\n",
    "\n",
    "\n",
    "        # Bornes pour éviter des valeurs aberrantes\n",
    "        self.stock_raw = max(0, min(10, self.stock_raw))\n",
    "        self.stock_sell = max(0, min(10, self.stock_sell))\n",
    "\n",
    "        # Incrémentation du temps\n",
    "        self.steps += 1\n",
    "        done = self.steps >= self.max_steps\n",
    "\n",
    "        next_state = (self.stock_raw, self.stock_sell)\n",
    "        return next_state, reward, done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e2afa75-3a41-4261-8627-1e0c0dbd16fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cellule 3 — Instanciation de l’environnement\n",
    "\n",
    "#But : créer une instance de l’atelier pour pouvoir interagir avec lui.\n",
    "#On crée l’environnement dans lequel l’agent va apprendre.\n",
    "\n",
    "env = SimpleWorkshopEnv(max_steps=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "838d4600-af8b-4dd9-ba04-372219b0f65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cellule 4 — Définition des tailles et création de la table-Q\n",
    "\n",
    "#But : créer la table-Q vide et définir les hyperparamètres du Q-learning.\n",
    "\n",
    "# Dimensions de l'espace d'états\n",
    "n_states_raw = 11      # 0 à 10\n",
    "n_states_sell = 11     # 0 à 10\n",
    "n_actions = 4          # 0,1,2,3\n",
    "\n",
    "# Table-Q initialisée à zéro\n",
    "Q = np.zeros((n_states_raw, n_states_sell, n_actions))\n",
    "\n",
    "# Hyperparamètres du Q-learning\n",
    "alpha = 0.1     # taux d'apprentissage\n",
    "gamma = 0.95    # importance du futur\n",
    "\n",
    "# Paramètres d'exploration\n",
    "epsilon = 1.0         # exploration maximale au début\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.995\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59a536e4-ae57-40b6-9acd-42854c2107b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cellule 5 — Fonction de choix d’action (ε-greedy)\n",
    "\n",
    "#But : choisir action aléatoire ou action optimale selon Q-table.\n",
    "#Cette cellule définit comment l’agent choisit ses actions : parfois au hasard, parfois selon la table-Q.\n",
    "\n",
    "def choose_action(state, epsilon):\n",
    "    stock_raw, stock_sell = state\n",
    "\n",
    "    # Exploration\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, n_actions - 1)\n",
    "\n",
    "    # Exploitation\n",
    "    return int(np.argmax(Q[stock_raw, stock_sell, :]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "343faef4-4260-48c4-9fc0-6d7c31a0d438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50 : Reward total = 273.0, epsilon = 0.778\n",
      "Episode 100 : Reward total = 364.0, epsilon = 0.606\n",
      "Episode 150 : Reward total = 389.0, epsilon = 0.471\n",
      "Episode 200 : Reward total = 431.0, epsilon = 0.367\n",
      "Episode 250 : Reward total = 488.0, epsilon = 0.286\n",
      "Episode 300 : Reward total = 516.0, epsilon = 0.222\n",
      "Episode 350 : Reward total = 542.0, epsilon = 0.173\n",
      "Episode 400 : Reward total = 556.0, epsilon = 0.135\n",
      "Episode 450 : Reward total = 553.0, epsilon = 0.105\n",
      "Episode 500 : Reward total = 573.0, epsilon = 0.100\n",
      "Episode 550 : Reward total = 556.0, epsilon = 0.100\n",
      "Episode 600 : Reward total = 601.0, epsilon = 0.100\n",
      "Episode 650 : Reward total = 612.0, epsilon = 0.100\n",
      "Episode 700 : Reward total = 621.0, epsilon = 0.100\n",
      "Episode 750 : Reward total = 615.0, epsilon = 0.100\n",
      "Episode 800 : Reward total = 544.0, epsilon = 0.100\n",
      "Episode 850 : Reward total = 650.0, epsilon = 0.100\n",
      "Episode 900 : Reward total = 559.0, epsilon = 0.100\n",
      "Episode 950 : Reward total = 560.0, epsilon = 0.100\n",
      "Episode 1000 : Reward total = 587.0, epsilon = 0.100\n",
      "Episode 1050 : Reward total = 594.0, epsilon = 0.100\n",
      "Episode 1100 : Reward total = 567.0, epsilon = 0.100\n",
      "Episode 1150 : Reward total = 598.0, epsilon = 0.100\n",
      "Episode 1200 : Reward total = 596.0, epsilon = 0.100\n",
      "Episode 1250 : Reward total = 632.0, epsilon = 0.100\n",
      "Episode 1300 : Reward total = 563.0, epsilon = 0.100\n",
      "Episode 1350 : Reward total = 639.0, epsilon = 0.100\n",
      "Episode 1400 : Reward total = 599.0, epsilon = 0.100\n",
      "Episode 1450 : Reward total = 637.0, epsilon = 0.100\n",
      "Episode 1500 : Reward total = 614.0, epsilon = 0.100\n",
      "Episode 1550 : Reward total = 637.0, epsilon = 0.100\n",
      "Episode 1600 : Reward total = 606.0, epsilon = 0.100\n",
      "Episode 1650 : Reward total = 561.0, epsilon = 0.100\n",
      "Episode 1700 : Reward total = 586.0, epsilon = 0.100\n",
      "Episode 1750 : Reward total = 611.0, epsilon = 0.100\n",
      "Episode 1800 : Reward total = 639.0, epsilon = 0.100\n",
      "Episode 1850 : Reward total = 561.0, epsilon = 0.100\n",
      "Episode 1900 : Reward total = 635.0, epsilon = 0.100\n",
      "Episode 1950 : Reward total = 578.0, epsilon = 0.100\n",
      "Episode 2000 : Reward total = 619.0, epsilon = 0.100\n",
      "Episode 2050 : Reward total = 587.0, epsilon = 0.100\n",
      "Episode 2100 : Reward total = 599.0, epsilon = 0.100\n",
      "Episode 2150 : Reward total = 594.0, epsilon = 0.100\n",
      "Episode 2200 : Reward total = 617.0, epsilon = 0.100\n",
      "Episode 2250 : Reward total = 574.0, epsilon = 0.100\n",
      "Episode 2300 : Reward total = 614.0, epsilon = 0.100\n",
      "Episode 2350 : Reward total = 578.0, epsilon = 0.100\n",
      "Episode 2400 : Reward total = 593.0, epsilon = 0.100\n",
      "Episode 2450 : Reward total = 619.0, epsilon = 0.100\n",
      "Episode 2500 : Reward total = 574.0, epsilon = 0.100\n",
      "Episode 2550 : Reward total = 579.0, epsilon = 0.100\n",
      "Episode 2600 : Reward total = 576.0, epsilon = 0.100\n",
      "Episode 2650 : Reward total = 588.0, epsilon = 0.100\n",
      "Episode 2700 : Reward total = 574.0, epsilon = 0.100\n",
      "Episode 2750 : Reward total = 619.0, epsilon = 0.100\n",
      "Episode 2800 : Reward total = 542.0, epsilon = 0.100\n",
      "Episode 2850 : Reward total = 617.0, epsilon = 0.100\n",
      "Episode 2900 : Reward total = 604.0, epsilon = 0.100\n",
      "Episode 2950 : Reward total = 571.0, epsilon = 0.100\n",
      "Episode 3000 : Reward total = 650.0, epsilon = 0.100\n",
      "Episode 3050 : Reward total = 599.0, epsilon = 0.100\n",
      "Episode 3100 : Reward total = 657.0, epsilon = 0.100\n",
      "Episode 3150 : Reward total = 583.0, epsilon = 0.100\n",
      "Episode 3200 : Reward total = 612.0, epsilon = 0.100\n",
      "Episode 3250 : Reward total = 599.0, epsilon = 0.100\n",
      "Episode 3300 : Reward total = 614.0, epsilon = 0.100\n",
      "Episode 3350 : Reward total = 556.0, epsilon = 0.100\n",
      "Episode 3400 : Reward total = 636.0, epsilon = 0.100\n",
      "Episode 3450 : Reward total = 594.0, epsilon = 0.100\n",
      "Episode 3500 : Reward total = 581.0, epsilon = 0.100\n",
      "Episode 3550 : Reward total = 614.0, epsilon = 0.100\n",
      "Episode 3600 : Reward total = 552.0, epsilon = 0.100\n",
      "Episode 3650 : Reward total = 616.0, epsilon = 0.100\n",
      "Episode 3700 : Reward total = 635.0, epsilon = 0.100\n",
      "Episode 3750 : Reward total = 524.0, epsilon = 0.100\n",
      "Episode 3800 : Reward total = 621.0, epsilon = 0.100\n",
      "Episode 3850 : Reward total = 563.0, epsilon = 0.100\n",
      "Episode 3900 : Reward total = 612.0, epsilon = 0.100\n",
      "Episode 3950 : Reward total = 650.0, epsilon = 0.100\n",
      "Episode 4000 : Reward total = 599.0, epsilon = 0.100\n",
      "Episode 4050 : Reward total = 617.0, epsilon = 0.100\n",
      "Episode 4100 : Reward total = 558.0, epsilon = 0.100\n",
      "Episode 4150 : Reward total = 637.0, epsilon = 0.100\n",
      "Episode 4200 : Reward total = 614.0, epsilon = 0.100\n",
      "Episode 4250 : Reward total = 580.0, epsilon = 0.100\n",
      "Episode 4300 : Reward total = 581.0, epsilon = 0.100\n",
      "Episode 4350 : Reward total = 580.0, epsilon = 0.100\n",
      "Episode 4400 : Reward total = 583.0, epsilon = 0.100\n",
      "Episode 4450 : Reward total = 583.0, epsilon = 0.100\n",
      "Episode 4500 : Reward total = 579.0, epsilon = 0.100\n",
      "Episode 4550 : Reward total = 614.0, epsilon = 0.100\n",
      "Episode 4600 : Reward total = 535.0, epsilon = 0.100\n",
      "Episode 4650 : Reward total = 577.0, epsilon = 0.100\n",
      "Episode 4700 : Reward total = 601.0, epsilon = 0.100\n",
      "Episode 4750 : Reward total = 594.0, epsilon = 0.100\n",
      "Episode 4800 : Reward total = 562.0, epsilon = 0.100\n",
      "Episode 4850 : Reward total = 632.0, epsilon = 0.100\n",
      "Episode 4900 : Reward total = 594.0, epsilon = 0.100\n",
      "Episode 4950 : Reward total = 601.0, epsilon = 0.100\n",
      "Episode 5000 : Reward total = 592.0, epsilon = 0.100\n"
     ]
    }
   ],
   "source": [
    "#Cellule 6 — Boucle d’apprentissage du Q-learning\n",
    "\n",
    "#But : exécuter plusieurs épisodes, mettre à jour la Q-table avec la formule de Bellman.\n",
    "#Cette cellule entraîne l’agent : il joue, reçoit des récompenses, met à jour la table-Q et apprend une stratégie.\n",
    "n_episodes = 5000\n",
    "rewards_per_episode = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # 1. Choisir une action\n",
    "        action = choose_action(state, epsilon)\n",
    "\n",
    "        # 2. Exécuter l'action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # 3. Indices pour la Q-table\n",
    "        sr, ss = state\n",
    "        nsr, nss = next_state\n",
    "\n",
    "        # 4. Q-learning update (formule de Bellman)\n",
    "        best_next_Q = np.max(Q[nsr, nss, :])\n",
    "        Q[sr, ss, action] += alpha * (reward + gamma * best_next_Q - Q[sr, ss, action])\n",
    "\n",
    "        # 5. Passer à l'état suivant\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    rewards_per_episode.append(total_reward)\n",
    "\n",
    "    # 6. Décroissance de epsilon\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    # Petit affichage\n",
    "    if (episode + 1) % 50 == 0:\n",
    "        print(f\"Episode {episode+1} : Reward total = {total_reward:.1f}, epsilon = {epsilon:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac82a95b-b404-45c6-b23e-dcc29b48753b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "État (0,0) → meilleure action = 3, Q-values = [131.72178299 161.24922249 138.44395731 237.77559485]\n",
      "État (1,0) → meilleure action = 3, Q-values = [126.92561584 108.25180142 123.99816928 239.94820801]\n",
      "État (2,0) → meilleure action = 2, Q-values = [142.7412958   31.54307784 253.76827934  88.62314424]\n",
      "État (3,0) → meilleure action = 3, Q-values = [177.25273938 165.16177712  86.57443537 255.18234946]\n",
      "État (4,0) → meilleure action = 2, Q-values = [141.13037211 195.8282422  241.62520086 159.1246792 ]\n",
      "État (5,0) → meilleure action = 2, Q-values = [180.42561544 227.1083797  255.55325773 173.41608801]\n",
      "État (6,0) → meilleure action = 2, Q-values = [222.54250593 232.62560199 257.84021896 236.57852096]\n",
      "État (7,0) → meilleure action = 2, Q-values = [224.57983038 115.20080539 271.07187299 150.94347478]\n",
      "État (8,0) → meilleure action = 2, Q-values = [220.76102325 198.9719982  273.87615733 224.03195213]\n",
      "État (9,0) → meilleure action = 2, Q-values = [101.90398826 113.18671373 254.60079761 214.74870957]\n",
      "État (10,0) → meilleure action = 2, Q-values = [216.88867731 101.65018808 271.70079761 158.95823283]\n"
     ]
    }
   ],
   "source": [
    "#Cellule 7 — Inspection de la politique apprise\n",
    "\n",
    "#But : afficher les meilleures actions apprises pour différents états.\n",
    "#Cette cellule permet de voir ce que l’agent a réellement appris comme stratégie.\n",
    "\n",
    "def best_action_for_state(stock_raw, stock_sell):\n",
    "    q_vals = Q[stock_raw, stock_sell, :]\n",
    "    best_a = int(np.argmax(q_vals))\n",
    "    return best_a, q_vals\n",
    "\n",
    "for sr in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    action, qv = best_action_for_state(sr, 0)\n",
    "    print(f\"État ({sr},0) → meilleure action = {action}, Q-values = {qv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cb6d9a-a5df-4324-a6b7-0c8ffce09948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:qlearning]",
   "language": "python",
   "name": "conda-env-qlearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
