{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "846bfca4-59cc-4121-9ee8-c837e1f79e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cellule 1 — Imports\n",
    "#But : importer les librairies nécessaires (NumPy pour les calculs, random pour le hasard).\n",
    "\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fc8e1bc-4685-43df-8740-174d896b658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cellule 2 — Nouvelle classe d’environnement avec durée des actions\n",
    "\n",
    "#Les règles deviendront :\n",
    "\n",
    "#P1 : récompense +2, consomme 1 matière, prend 1 unité de temps\n",
    "\n",
    "#P2 : récompense +20, consomme 2 matières, prend 3 unités de temps\n",
    "\n",
    "#Commander : récompense −5, ajoute 5 matières, prend 1 unité de temps\n",
    "\n",
    "#Attendre : 0 ou −1 si stock=0, prend 1 unité de temps\n",
    "\n",
    "\n",
    "class WorkshopEnvV2:\n",
    "    \"\"\"\n",
    "    Environnement d'atelier avec durée variable des actions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_steps=50):\n",
    "        self.max_steps = max_steps\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Stock initial aléatoire entre 0 et 10\n",
    "        self.stock_raw = np.random.randint(0, 11)\n",
    "        self.stock_sell = 0\n",
    "        \n",
    "        # Compteur de temps\n",
    "        self.steps = 0\n",
    "        \n",
    "        return (self.stock_raw, self.stock_sell)\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        \n",
    "        # Durées des actions\n",
    "        duration_P1 = 1\n",
    "        duration_P2 = 3\n",
    "        duration_cmd = 1\n",
    "        duration_wait = 1\n",
    "\n",
    "        # ACTION 1 : Produire Produit 1\n",
    "        if action == 1:\n",
    "            if self.stock_raw >= 1:\n",
    "                self.stock_raw -= 1\n",
    "                self.stock_sell += 1\n",
    "                reward = 2\n",
    "            self.steps += duration_P1\n",
    "\n",
    "        # ACTION 2 : Produire Produit 2\n",
    "        elif action == 2:\n",
    "            if self.stock_raw >= 2:\n",
    "                self.stock_raw -= 2\n",
    "                self.stock_sell += 1\n",
    "                reward = 20\n",
    "            self.steps += duration_P2\n",
    "\n",
    "        # ACTION 3 : Commander\n",
    "        elif action == 3:\n",
    "            self.stock_raw += 5\n",
    "            reward = -5\n",
    "            self.steps += duration_cmd\n",
    "\n",
    "        # ACTION 0 : Attendre\n",
    "        elif action == 0:\n",
    "            reward = -1 if self.stock_raw == 0 else 0\n",
    "            self.steps += duration_wait\n",
    "\n",
    "        # BORNES\n",
    "        self.stock_raw = max(0, min(10, self.stock_raw))\n",
    "        self.stock_sell = max(0, min(10, self.stock_sell))\n",
    "\n",
    "        # FIN D'ÉPISODE\n",
    "        done = self.steps >= self.max_steps\n",
    "\n",
    "        next_state = (self.stock_raw, self.stock_sell)\n",
    "        return next_state, reward, done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c6403be-a708-46ef-a78f-e66886307fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 3 — Instanciation de l'environnement\n",
    "env = WorkshopEnvV2(max_steps=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29366052-fefe-41be-8b43-71706cb58dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 4 — Table-Q et hyperparamètres\n",
    "\n",
    "n_states_raw = 11\n",
    "n_states_sell = 11\n",
    "n_actions = 4\n",
    "\n",
    "Q = np.zeros((n_states_raw, n_states_sell, n_actions))\n",
    "\n",
    "alpha = 0.1\n",
    "gamma = 0.95\n",
    "\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.995\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "534bbe44-81cb-48cd-a53f-ce21dbee4bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 5 — Choix d'action ε-greedy\n",
    "\n",
    "def choose_action(state, epsilon):\n",
    "    stock_raw, stock_sell = state\n",
    "    \n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, n_actions - 1)\n",
    "    \n",
    "    return int(np.argmax(Q[stock_raw, stock_sell, :]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a91bf1f5-96f9-414b-8565-ede8bd6543ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 : Reward total = 196.0, epsilon = 0.606\n",
      "Episode 200 : Reward total = 237.0, epsilon = 0.367\n",
      "Episode 300 : Reward total = 237.0, epsilon = 0.222\n",
      "Episode 400 : Reward total = 254.0, epsilon = 0.135\n",
      "Episode 500 : Reward total = 254.0, epsilon = 0.100\n",
      "Episode 600 : Reward total = 238.0, epsilon = 0.100\n",
      "Episode 700 : Reward total = 261.0, epsilon = 0.100\n",
      "Episode 800 : Reward total = 256.0, epsilon = 0.100\n",
      "Episode 900 : Reward total = 277.0, epsilon = 0.100\n",
      "Episode 1000 : Reward total = 259.0, epsilon = 0.100\n",
      "Episode 1100 : Reward total = 238.0, epsilon = 0.100\n",
      "Episode 1200 : Reward total = 254.0, epsilon = 0.100\n",
      "Episode 1300 : Reward total = 253.0, epsilon = 0.100\n",
      "Episode 1400 : Reward total = 272.0, epsilon = 0.100\n",
      "Episode 1500 : Reward total = 253.0, epsilon = 0.100\n",
      "Episode 1600 : Reward total = 280.0, epsilon = 0.100\n",
      "Episode 1700 : Reward total = 261.0, epsilon = 0.100\n",
      "Episode 1800 : Reward total = 258.0, epsilon = 0.100\n",
      "Episode 1900 : Reward total = 275.0, epsilon = 0.100\n",
      "Episode 2000 : Reward total = 232.0, epsilon = 0.100\n",
      "Episode 2100 : Reward total = 256.0, epsilon = 0.100\n",
      "Episode 2200 : Reward total = 231.0, epsilon = 0.100\n",
      "Episode 2300 : Reward total = 250.0, epsilon = 0.100\n",
      "Episode 2400 : Reward total = 277.0, epsilon = 0.100\n",
      "Episode 2500 : Reward total = 256.0, epsilon = 0.100\n",
      "Episode 2600 : Reward total = 238.0, epsilon = 0.100\n",
      "Episode 2700 : Reward total = 277.0, epsilon = 0.100\n",
      "Episode 2800 : Reward total = 264.0, epsilon = 0.100\n",
      "Episode 2900 : Reward total = 275.0, epsilon = 0.100\n",
      "Episode 3000 : Reward total = 244.0, epsilon = 0.100\n",
      "Episode 3100 : Reward total = 259.0, epsilon = 0.100\n",
      "Episode 3200 : Reward total = 251.0, epsilon = 0.100\n",
      "Episode 3300 : Reward total = 254.0, epsilon = 0.100\n",
      "Episode 3400 : Reward total = 275.0, epsilon = 0.100\n",
      "Episode 3500 : Reward total = 234.0, epsilon = 0.100\n",
      "Episode 3600 : Reward total = 259.0, epsilon = 0.100\n",
      "Episode 3700 : Reward total = 277.0, epsilon = 0.100\n",
      "Episode 3800 : Reward total = 254.0, epsilon = 0.100\n",
      "Episode 3900 : Reward total = 220.0, epsilon = 0.100\n",
      "Episode 4000 : Reward total = 275.0, epsilon = 0.100\n",
      "Episode 4100 : Reward total = 272.0, epsilon = 0.100\n",
      "Episode 4200 : Reward total = 275.0, epsilon = 0.100\n",
      "Episode 4300 : Reward total = 275.0, epsilon = 0.100\n",
      "Episode 4400 : Reward total = 275.0, epsilon = 0.100\n",
      "Episode 4500 : Reward total = 236.0, epsilon = 0.100\n",
      "Episode 4600 : Reward total = 275.0, epsilon = 0.100\n",
      "Episode 4700 : Reward total = 253.0, epsilon = 0.100\n",
      "Episode 4800 : Reward total = 254.0, epsilon = 0.100\n",
      "Episode 4900 : Reward total = 243.0, epsilon = 0.100\n",
      "Episode 5000 : Reward total = 238.0, epsilon = 0.100\n"
     ]
    }
   ],
   "source": [
    "# Cellule 6 — Boucle d'entraînement\n",
    "\n",
    "n_episodes = 5000\n",
    "rewards_per_episode = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = choose_action(state, epsilon)\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        sr, ss = state\n",
    "        nsr, nss = next_state\n",
    "        \n",
    "        best_next_Q = np.max(Q[nsr, nss, :])\n",
    "        \n",
    "        Q[sr, ss, action] += alpha * (reward + gamma * best_next_Q - Q[sr, ss, action])\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    rewards_per_episode.append(total_reward)\n",
    "\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "        \n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(f\"Episode {episode+1} : Reward total = {total_reward:.1f}, epsilon = {epsilon:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15087e66-b958-498a-98d5-604df52a0565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "État (0,0) → meilleure action = 3, Q-values = [127.59260482 137.7296523  142.95576942 223.70320792]\n",
      "État (1,0) → meilleure action = 1, Q-values = [119.85153479 235.76827929 121.26607758 122.64238634]\n",
      "État (2,0) → meilleure action = 2, Q-values = [131.42953518  72.51343674 253.76827928 132.13549047]\n",
      "État (3,0) → meilleure action = 2, Q-values = [135.14609144 141.71911636 233.25812505 153.67407052]\n",
      "État (4,0) → meilleure action = 2, Q-values = [142.60385798 124.47999572 250.35812512 153.76076923]\n",
      "État (5,0) → meilleure action = 2, Q-values = [196.09473249 182.66384224 240.74021893 205.65638345]\n",
      "État (6,0) → meilleure action = 2, Q-values = [123.7857802  190.7755053  257.84021874 139.70088679]\n",
      "État (7,0) → meilleure action = 2, Q-values = [217.78324882 205.22542495 271.07187296 159.59015883]\n",
      "État (8,0) → meilleure action = 2, Q-values = [152.12881229 122.18678779 256.50051889 144.98685224]\n",
      "État (9,0) → meilleure action = 2, Q-values = [168.58618217 186.20146713 266.23223699 109.3403735 ]\n",
      "État (10,0) → meilleure action = 2, Q-values = [209.03016982  84.08512901 280.89334926 173.40415111]\n"
     ]
    }
   ],
   "source": [
    "# Cellule 7 — Inspection de la politique\n",
    "\n",
    "def best_action_for_state(stock_raw, stock_sell):\n",
    "    q_vals = Q[stock_raw, stock_sell, :]\n",
    "    best_a = int(np.argmax(q_vals))\n",
    "    return best_a, q_vals\n",
    "\n",
    "for sr in range(0, 11):\n",
    "    action, qv = best_action_for_state(sr, 0)\n",
    "    print(f\"État ({sr},0) → meilleure action = {action}, Q-values = {qv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cb6d9a-a5df-4324-a6b7-0c8ffce09948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:qlearning]",
   "language": "python",
   "name": "conda-env-qlearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
