{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e871695-8974-4524-bf98-a5c301bf7bc1",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e5242dd-494f-472a-8c94-3c3f145a290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "import simpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d10bcae-3ff5-4c9c-a9c2-40f7623e1851",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "class GridWorldEnv(gym.Env):\n",
    "\n",
    "    def __init__(self, size: int = 5):\n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "            {\n",
    "                \"agent\": gym.spaces.Box(0, size - 1, shape=(2,), dtype=int),   # [x, y] coordinates\n",
    "                \"target\": gym.spaces.Box(0, size - 1, shape=(2,), dtype=int),  # [x, y] coordinates\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Define what actions are available (4 directions)\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"Convert internal state to observation format.\n",
    "\n",
    "        Returns:\n",
    "            dict: Observation with agent and target positions\n",
    "        \"\"\"\n",
    "        return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "\n",
    "    def _get_info(self):\n",
    "        \"\"\"Compute auxiliary information for debugging.\n",
    "\n",
    "        Returns:\n",
    "            dict: Info with distance between agent and target\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"distance\": np.linalg.norm(\n",
    "                self._agent_location - self._target_location, ord=1\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        \"\"\"Start a new episode.\n",
    "\n",
    "        Args:\n",
    "            seed: Random seed for reproducible episodes\n",
    "            options: Additional configuration (unused in this example)\n",
    "\n",
    "        Returns:\n",
    "            tuple: (observation, info) for the initial state\n",
    "        \"\"\"\n",
    "        # IMPORTANT: Must call this first to seed the random number generator\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Execute one timestep within the environment.\n",
    "\n",
    "        Args:\n",
    "            action: The action to take (0-3 for directions)\n",
    "\n",
    "        Returns:\n",
    "            tuple: (observation, reward, terminated, truncated, info)\n",
    "        \"\"\"\n",
    "        # Map the discrete action (0-3) to a movement direction\n",
    "        direction = self._action_to_direction[action]\n",
    "\n",
    "        # Update agent position, ensuring it stays within grid bounds\n",
    "        # np.clip prevents the agent from walking off the edge\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction, 0, self.size - 1\n",
    "        )\n",
    "\n",
    "        # Check if agent reached the target\n",
    "        terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "\n",
    "        # We don't use truncation in this simple environment\n",
    "        # (could add a step limit here if desired)\n",
    "        truncated = False\n",
    "\n",
    "        # Simple reward structure: +1 for reaching target, 0 otherwise\n",
    "        # Alternative: could give small negative rewards for each step to encourage efficiency\n",
    "        reward = 1 if terminated else 0\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeb249b-a81e-413c-9f66-0991b8850393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepRL1",
   "language": "python",
   "name": "deeprl1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
